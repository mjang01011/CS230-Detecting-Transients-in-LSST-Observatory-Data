{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning for RNN/LSTM on Light Curve Data\n",
    "Grid search over hyperparameters for both RNN and LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mjang01011/CS230-Detecting-Transients-in-LSST-Observatory-Data.git\n",
    "%cd CS230-Detecting-Transients-in-LSST-Observatory-Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "!mkdir -p data\n",
    "!mv processed_training.csv data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/processed_training.csv')\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Unique objects: {df['object_id'].nunique()}\")\n",
    "print(f\"Unique targets: {sorted(df['target'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from lib.dataset import LightCurveDataset\n",
    "from models.rnn import LightCurveRNN\n",
    "from models.lstm import LightCurveLSTM\n",
    "from models.gru import LightCurveGRU\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'model': ['rnn', 'lstm', 'gru'],\n",
    "    'hidden_size': [64, 128, 256],\n",
    "    'num_layers': [2, 3, 4, 5],\n",
    "    'lr': [0.1, 0.01, 0.001, 0.0005],\n",
    "    'batch_size': [64],\n",
    "    'max_length': [200]\n",
    "}\n",
    "\n",
    "# Fixed training params\n",
    "EPOCHS = 15\n",
    "DATA_PATH = 'data/processed_training.csv'\n",
    "\n",
    "print(f\"Total combinations: {len(list(product(*param_grid.values())))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_acc = 0.0\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_data, batch_labels in train_loader:\n",
    "            batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_data)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "            train_total += batch_labels.size(0)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_data, batch_labels in val_loader:\n",
    "                batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "                \n",
    "                outputs = model(batch_data)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                predicted = outputs.argmax(dim=1)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "                val_total += batch_labels.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    return best_val_acc, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset once\n",
    "dataset = LightCurveDataset(DATA_PATH, max_length=200, use_flux_only=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Use generator for reproducible split\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "num_classes = dataset.num_classes\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"Train: {train_size}, Val: {val_size}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Seed: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "results = []\n",
    "best_models = {'rnn': {'val_acc': 0, 'params': None, 'history': None},\n",
    "               'lstm': {'val_acc': 0, 'params': None, 'history': None},\n",
    "               'gru': {'val_acc': 0, 'params': None, 'history': None}}\n",
    "\n",
    "total_runs = len(list(product(*param_grid.values())))\n",
    "current_run = 0\n",
    "\n",
    "for params in product(*param_grid.values()):\n",
    "    current_run += 1\n",
    "    model_type, hidden_size, num_layers, lr, batch_size, max_length = params\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Run {current_run}/{total_runs}\")\n",
    "    print(f\"Model: {model_type}, Hidden: {hidden_size}, Layers: {num_layers}, LR: {lr}, Batch: {batch_size}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Set seed before each run for reproducibility\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    # Create dataloaders with fixed seed\n",
    "    generator = torch.Generator().manual_seed(SEED)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    if model_type == 'rnn':\n",
    "        model = LightCurveRNN(input_size=1, hidden_size=hidden_size, \n",
    "                             num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "    elif model_type == 'lstm':\n",
    "        model = LightCurveLSTM(input_size=1, hidden_size=hidden_size, \n",
    "                              num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "    elif model_type == 'gru':\n",
    "        model = LightCurveGRU(input_size=1, hidden_size=hidden_size, \n",
    "                             num_layers=num_layers, num_classes=num_classes).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_type}\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    best_val_acc, history = train_model(model, train_loader, val_loader, \n",
    "                                       criterion, optimizer, EPOCHS, device)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Record results\n",
    "    result = {\n",
    "        'model': model_type,\n",
    "        'hidden_size': hidden_size,\n",
    "        'num_layers': num_layers,\n",
    "        'lr': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'max_length': max_length,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'training_time': training_time,\n",
    "        'final_train_loss': history[-1]['train_loss'],\n",
    "        'final_train_acc': history[-1]['train_acc'],\n",
    "        'final_val_loss': history[-1]['val_loss'],\n",
    "        'final_val_acc': history[-1]['val_acc'],\n",
    "        'seed': SEED\n",
    "    }\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(f\"Training Time: {training_time:.2f}s\")\n",
    "    \n",
    "    # Save best model and history\n",
    "    if best_val_acc > best_models[model_type]['val_acc']:\n",
    "        best_models[model_type]['val_acc'] = best_val_acc\n",
    "        best_models[model_type]['params'] = result.copy()\n",
    "        best_models[model_type]['history'] = history\n",
    "        torch.save(model.state_dict(), f'best_{model_type}_model.pth')\n",
    "        print(f\"*** New best {model_type.upper()} model saved! ***\")\n",
    "    \n",
    "    # Save intermediate results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('hyperparameter_results.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Grid Search Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "results_df = pd.read_csv('hyperparameter_results.csv')\n",
    "results_df = results_df.sort_values('best_val_acc', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Configurations:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Best RNN Model:\")\n",
    "print(\"=\"*80)\n",
    "best_rnn = results_df[results_df['model'] == 'rnn'].iloc[0]\n",
    "print(best_rnn)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Best LSTM Model:\")\n",
    "print(\"=\"*80)\n",
    "best_lstm = results_df[results_df['model'] == 'lstm'].iloc[0]\n",
    "print(best_lstm)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Best GRU Model:\")\n",
    "print(\"=\"*80)\n",
    "best_gru = results_df[results_df['model'] == 'gru'].iloc[0]\n",
    "print(best_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves for best models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "for idx, model_type in enumerate(['rnn', 'lstm', 'gru']):\n",
    "    history = best_models[model_type]['history']\n",
    "    \n",
    "    if history is None:\n",
    "        print(f\"No history found for {model_type}\")\n",
    "        continue\n",
    "    \n",
    "    epochs = [h['epoch'] for h in history]\n",
    "    train_losses = [h['train_loss'] for h in history]\n",
    "    val_losses = [h['val_loss'] for h in history]\n",
    "    \n",
    "    axes[idx].plot(epochs, train_losses, label='Train Loss', marker='o', linewidth=2)\n",
    "    axes[idx].plot(epochs, val_losses, label='Val Loss', marker='s', linewidth=2)\n",
    "    axes[idx].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[idx].set_ylabel('Loss', fontsize=12)\n",
    "    axes[idx].set_title(f'Best {model_type.upper()} Model - Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[idx].legend(fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    params = best_models[model_type]['params']\n",
    "    info_text = f\"Hidden: {params['hidden_size']}, Layers: {params['num_layers']}\\nLR: {params['lr']}, Batch: {params['batch_size']}\\nBest Val Acc: {params['best_val_acc']:.2f}%\"\n",
    "    axes[idx].text(0.02, 0.98, info_text, transform=axes[idx].transAxes, \n",
    "                   fontsize=9, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('best_models_training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest Model Parameters:\")\n",
    "for model_type in ['rnn', 'lstm', 'gru']:\n",
    "    params = best_models[model_type]['params']\n",
    "    print(f\"\\n{model_type.upper()}:\")\n",
    "    print(f\"  Hidden Size: {params['hidden_size']}\")\n",
    "    print(f\"  Num Layers: {params['num_layers']}\")\n",
    "    print(f\"  Learning Rate: {params['lr']}\")\n",
    "    print(f\"  Batch Size: {params['batch_size']}\")\n",
    "    print(f\"  Best Val Acc: {params['best_val_acc']:.2f}%\")\n",
    "    print(f\"  Final Train Loss: {params['final_train_loss']:.4f}\")\n",
    "    print(f\"  Final Val Loss: {params['final_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results and models\n",
    "from google.colab import files\n",
    "\n",
    "files.download('hyperparameter_results.csv')\n",
    "files.download('best_models_training_curves.png')\n",
    "files.download('best_rnn_model.pth')\n",
    "files.download('best_lstm_model.pth')\n",
    "files.download('best_gru_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
